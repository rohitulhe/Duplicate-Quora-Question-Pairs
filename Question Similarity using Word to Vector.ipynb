{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data and removing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of base training File =  (20000, 6)\n",
      "Shape of base training data after cleaning =  (20000, 6)\n",
      "          id   qid1   qid2                                          question1  \\\n",
      "13695  13695  26277  26278  What is the expected cut off NTSE stage 1 up 2...   \n",
      "8905    8905  17334   3572                   How can one learn Japanese well?   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "13695  What is the expected NTSE stage 1 UP cut off?             1  \n",
      "8905                     How did you learn Japanese?             0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data():                                        # creating a function named read_data()\n",
    "    df = pd.read_csv(\"train.csv\", nrows=20000)\n",
    "    print (\"Shape of base training File = \", df.shape)\n",
    "    df.drop_duplicates(inplace=True)                    # Remove missing values and duplicates from training data\n",
    "    df.dropna(inplace=True)\n",
    "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
    "    return df\n",
    "df = read_data()\n",
    "df_train, df_test = train_test_split(df, test_size = 0.02)\n",
    "print (df_train.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the duplicate values counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Count = 7334 , Non Duplicate Count = 12266\n",
      "Unique Questions = 37063\n",
      "Count of Quesitons appearing more than once = 1733\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "def eda(df):                                        # defining a function to see unique questionsssss\n",
    "    print (\"Duplicate Count = %d , Non Duplicate Count = %d\" \n",
    "           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n",
    "    \n",
    "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
    "    \n",
    "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
    "    \n",
    "    question_ids_counter = Counter(question_ids_combined) #is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values\n",
    "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
    "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
    "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n",
    "    \n",
    "    \n",
    "eda(df_train)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions to tokenize words in the question and filtering extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of words in the dictionary = 4838\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "words = re.compile(r\"\\w+\",re.I)         # creating an object and ignoring the case for matching and searching of words.\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()               #  is a process for removing the commoner morphological and inflexional endings from words in English.\n",
    "\n",
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)  #able to store a collection of raw text documents with additional key-value headers.\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()                               # Assigns new ids to the word\n",
    "    \n",
    "    return dictionary\n",
    "# Calling Function\n",
    "df_train = tokenize_questions(df_train)\n",
    "dictionary = train_dictionary(df_train)\n",
    "print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "df_test = tokenize_questions(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function named get_vector to convert documents into bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19600, 4838)\n",
      "(19600, 4838)\n"
     ]
    }
   ],
   "source": [
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()] #Convert document into the bag-of-words\n",
    "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
    "    \n",
    "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
    "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return question1_csc.transpose(),question2_csc.transpose()\n",
    "\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q2_csc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating different similarity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim sample= \n",
      " [0.85714285714285687, 0.7745966692414834]\n",
      "manhattan_dis sample = \n",
      " [2.0, 2.0]\n",
      "eucledian_dis sample = \n",
      " [1.4142135623730951, 1.4142135623730951]\n",
      "jaccard_dis sample = \n",
      " [0.75, 0.59999999999999998]\n",
      "minkowsk_dis sample = \n",
      " [1.4142135623730951, 1.4142135623730951]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Minkowski_distance \n",
    "# https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "\n",
    "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
    "mms_scale_man = MinMaxScaler()\n",
    "mms_scale_euc = MinMaxScaler()\n",
    "mms_scale_mink = MinMaxScaler()\n",
    "\n",
    "def get_similarity_values(q1_csc, q2_csc): # defining a function to calculate similarity values\n",
    "    cosine_sim = []                        \n",
    "    manhattan_dis = []\n",
    "    eucledian_dis = []\n",
    "    jaccard_dis = []\n",
    "    minkowsk_dis = []\n",
    "    \n",
    "    for i,j in zip(q1_csc, q2_csc):\n",
    "        sim = cs(i,j)\n",
    "        cosine_sim.append(sim[0][0])\n",
    "        sim = md(i,j)\n",
    "        manhattan_dis.append(sim[0][0])\n",
    "        sim = ed(i,j)\n",
    "        eucledian_dis.append(sim[0][0])\n",
    "        i_ = i.toarray()\n",
    "        j_ = j.toarray()\n",
    "        try:\n",
    "            sim = jsc(i_,j_)\n",
    "            jaccard_dis.append(sim)\n",
    "        except:\n",
    "            jaccard_dis.append(0)\n",
    "            \n",
    "        sim = minkowski_dis.pairwise(i_,j_)\n",
    "        minkowsk_dis.append(sim[0][0])\n",
    "    \n",
    "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
    "\n",
    "\n",
    "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
    "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
    "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
    "print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
    "print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
    "print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
    "print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
    "\n",
    "eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
    "manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
    "minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
    "    \n",
    "manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
    "eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
    "minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
    "\n",
    "eucledian_dis = eucledian_dis_array.flatten()\n",
    "manhattan_dis = manhattan_dis_array.flatten()\n",
    "minkowsk_dis = minkowsk_dis_array.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the log loss values for the similarity values calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated log loss value on the test set for cosine sim is = 1.569353\n",
      "The calculated log loss value on the test set for manhattan sim is = 2.327256\n",
      "The calculated log loss value on the test set for euclidean sim is = 1.964779\n",
      "The calculated log loss value on the test set for jaccard sim is = 3.265609\n",
      "The calculated log loss value on the test set for minkowski sim is = 1.964779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calculate_logloss(y_true, y_pred):     # defining a function to calculate logloss\n",
    "    loss_cal = log_loss(y_true, y_pred)\n",
    "    return loss_cal\n",
    "\n",
    "q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
    "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
    "y_true = df_test.is_duplicate.tolist()\n",
    "\n",
    "y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
    "y_pred_man = y_pred_man_array.flatten()\n",
    "\n",
    "y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
    "y_pred_euc = y_pred_euc_array.flatten()\n",
    "\n",
    "y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
    "y_pred_mink = y_pred_mink_array.flatten()\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_cos)\n",
    "print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_man)\n",
    "print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_euc)\n",
    "print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_jac)\n",
    "print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_mink)\n",
    "print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the parameters of Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
    "y_train = df_train.is_duplicate\n",
    "\n",
    "X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
    "y_test = y_true\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating log loss values using Random Forest Regressor and Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated log loss value on the test set using RFR is = 1.232292\n",
      "The calculated log loss value on the test set using SVR is = 0.696790\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Random_forest\n",
    "# http://www.saedsayad.com/support_vector_machine_reg.htm\n",
    "y_rfr_predicted = rfr.predict(X_test)\n",
    "y_svr_predicted = svr.predict(X_test)\n",
    "\n",
    "logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n",
    "logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n",
    "\n",
    "print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n",
    "print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The text in the document by <Shireen Rabbani, Rohit Ulhe> is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
    "The code in the document by <Shireen Rabbani, Rohit Ulhe> is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
